{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn import svm, preprocessing, pipeline\n",
    "import sklearn.linear_model as LM\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "\n",
    "import xgboost as xgb\n",
    "from xgboost import XGBRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from xgboost import cv\n",
    "from scipy.stats import pearsonr\n",
    "import lightgbm as lgb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model development below\n",
    "Code is scattered, but the goal is to build a run_model function that takes in data, data type, and drug and returns single table comparing all models. Then the goal will be to iterate through data type combos and drugs and come up with a complete analysis output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from load_data import AMLData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# point of access for all data\n",
    "data = AMLData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "en_model = LM.ElasticNet( \n",
    "    random_state=0,\n",
    "    max_iter=100000, \n",
    "    fit_intercept=True,\n",
    "    l1_ratio=.7,\n",
    "    alpha=.9,\n",
    ")\n",
    "\n",
    "lasso_model = LM.Lasso(\n",
    "    alpha=0.1,\n",
    "    max_iter=100000, \n",
    "    fit_intercept=True,\n",
    ")\n",
    "\n",
    "svm_model = svm.SVR(C=1.0, epsilon=0.2, kernel='linear')\n",
    "svc_model = svm.SVC(C=1.0, kernel='linear')\n",
    "\n",
    "def run_sklearn(x_train, y_train, x_test, y_test, model, model_name, binarize=False):\n",
    "        \n",
    "    # don't think we need scaling? Can add in pretty quickly if we do\n",
    "#     pipe = pipeline.Pipeline([\n",
    "# #         ('scaler', preprocessing.StandardScaler()),\n",
    "#         ('model', model)\n",
    "#     ])\n",
    "    if binarize:\n",
    "        y_train, y_test = convert_to_binary(y_train, y_test)\n",
    "    \n",
    "    model.fit(x_train, y_train)\n",
    "    train_pred = model.predict(x_train)\n",
    "    preds = model.predict(x_test)\n",
    "    error, r2, pearson = score_all(y_test, preds)\n",
    "    coef = np.array(model.coef_).flatten()\n",
    "    feature_names = model.feature_names_in_[coef>0]\n",
    "    auc = np.nan\n",
    "    if binarize:\n",
    "        auc = metrics.average_precision_score(y_test, preds)\n",
    "    return {\n",
    "        'test_prediction': preds,\n",
    "        'train_prediction': train_pred,\n",
    "        'pearsonr': pearsonr(y_test, preds)[0],\n",
    "        'mse': error,\n",
    "        'r2' : r2,\n",
    "        'model': model_name,\n",
    "        'feature_names': feature_names,\n",
    "        'auc': auc\n",
    "    }\n",
    "\n",
    "\n",
    "def convert_to_binary(y_train, y_test):\n",
    "    \"\"\" Binarize AUC \"\"\"\n",
    "    y_train_c = np.copy(y_train)\n",
    "    y_test_c = np.copy(y_test)\n",
    "    y_train_c[y_train_c<100] = 1\n",
    "    y_train_c[y_train_c>100] = 0\n",
    "    y_test_c[y_test_c<100] = 1\n",
    "    y_test_c[y_test_c>100] = 0\n",
    "    return y_train_c, y_test_c\n",
    "\n",
    "\n",
    "def run_gbt(x_train, y_train, x_test, y_test, feature_names, binarize=False):\n",
    "    \n",
    "    # again, don't know if we need to scale. Doesn't seem to have an impact...\n",
    "#     scaler = preprocessing.StandardScaler()\n",
    "#     x_train = scaler.fit_transform(x_train)\n",
    "#     x_test = scaler.fit_transform(x_test)\n",
    "\n",
    "\n",
    "    \n",
    "    param = dict(\n",
    "        device_type = 'cpu',\n",
    "        boosting = 'gbdt',\n",
    "        nthread = 1,\n",
    "        objective = 'regression',\n",
    "        metric = 'rmse',\n",
    "        lambda_l1=1,\n",
    "        lambda_l2=1,\n",
    "        learning_rate = .01,\n",
    "        tree_learner = 'serial',\n",
    "        max_bin = 63,\n",
    "        num_leaves = 10, \n",
    "        max_depth = 10,\n",
    "        feature_fraction = .5,\n",
    "        min_data_in_leaf = 1,\n",
    "        min_gain_to_split = 1,\n",
    "        verbose = -1\n",
    "        \n",
    "    )\n",
    "    model_name = 'gbt'\n",
    "    \n",
    "    if binarize:\n",
    "        param['objective'] = 'binary'\n",
    "        param['metric'] = 'auc'\n",
    "        y_train, y_test = convert_to_binary(y_train, y_test)\n",
    "        model_name = 'gbt_binary'\n",
    "        \n",
    "    train_data = lgb.Dataset(x_train, label=y_train, feature_name=feature_names)\n",
    "    validation_data = lgb.Dataset(x_test, label=y_test, feature_name=feature_names)\n",
    "    num_round = 1000\n",
    "    bst = lgb.train(\n",
    "        param, \n",
    "        train_data, \n",
    "        num_round, \n",
    "        valid_sets=validation_data,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100, verbose=0)]\n",
    "    )\n",
    "    table = bst.feature_importance()\n",
    "    feats = pd.Series(table, index=feature_names)\n",
    "    selected_feat = feats[feats > 0].index.values\n",
    "    \n",
    "    train_pred = bst.predict(x_train, num_iteration=bst.best_iteration)\n",
    "    preds = bst.predict(x_test, num_iteration=bst.best_iteration)\n",
    "    error, r2, pearson = score_all(y_test, preds)\n",
    "    auc = np.nan\n",
    "    if binarize:\n",
    "        auc = metrics.average_precision_score(y_test, preds)\n",
    "    return {\n",
    "        'test_prediction': preds,\n",
    "        'train_prediction': train_pred,\n",
    "        'mse': error,\n",
    "        'r2' : r2,\n",
    "        'pearsonr': pearsonr(y_test, preds)[0],\n",
    "        'model': model_name,\n",
    "        'feature_names': selected_feat,\n",
    "        'auc': auc\n",
    "    }\n",
    "def score_all(y_test, preds):\n",
    "    error = np.sqrt(metrics.mean_squared_error(y_test, preds))\n",
    "    r2 = metrics.r2_score(y_test, preds)\n",
    "    pearson = pearsonr(y_test, preds)[0]\n",
    "#     print(f\"RMSE: {error:0.3f} | R^2 {r2:0.3f} | R {pearson:0.3f}\")\n",
    "    return error, r2, pearson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_model(data, d_sets, drug_name):\n",
    "    df_subset = data.get_trainable_data(d_sets, drug_name)\n",
    "    cols = list(set(df_subset.columns.values))\n",
    "    cols.remove(drug_name)\n",
    "\n",
    "    features = df_subset[cols].copy()\n",
    "    target = df_subset[drug_name].values\n",
    "    \n",
    "    n_features_before = features.shape[1]\n",
    "    \n",
    "#     selector = VarianceThreshold(threshold=.001)\n",
    "#     features = selector.fit_transform(features)\n",
    "#     feature_names = selector.get_feature_names_out()\n",
    "    \n",
    "    features = features.loc[:, features.mean() > 0]\n",
    "    features = features.loc[:, features.std() > 0]\n",
    "    n_features = features.shape[1]\n",
    "    print(f\"Using {n_features} out of {n_features_before}\"\n",
    "          f\" ({n_features_before - n_features} removed)\")\n",
    "    \n",
    "#     features = pd.DataFrame(features, columns=feature_names)\n",
    "    feature_names = list(set(features.columns.values))\n",
    "\n",
    "    x_train, x_test, y_train, y_test = train_test_split(\n",
    "        features,\n",
    "        target,\n",
    "        test_size=0.2,\n",
    "        shuffle=True,\n",
    "        random_state=101,\n",
    "    )\n",
    "    \n",
    "    gbt_results = run_gbt(x_train, y_train, x_test, y_test, feature_names)\n",
    "    gbt_binary_results = run_gbt(x_train, y_train, x_test, y_test, feature_names, binarize=True)\n",
    "    enet_results = run_sklearn(x_train, y_train, x_test, y_test, en_model, 'EN')\n",
    "    lasso_results = run_sklearn(x_train, y_train, x_test, y_test, lasso_model, 'LASSO')\n",
    "    svm_results = run_sklearn(x_train, y_train, x_test, y_test, svm_model, 'SVM')\n",
    "    svc_results = run_sklearn(x_train, y_train, x_test, y_test, svm_model, 'SVC', binarize=True)\n",
    "\n",
    "    avg = np.zeros(len(enet_results['test_prediction']))\n",
    "    for i in [gbt_results, enet_results, lasso_results, svm_results]:\n",
    "        sns.regplot(y_test, i['test_prediction'], label=i['model'])\n",
    "        avg += i['test_prediction']\n",
    "    avg = avg/4\n",
    "    score_all(y_test, avg)\n",
    "    sns.regplot(y_test, avg, label='Average')\n",
    "    plt.xlabel(\"Actual\")\n",
    "    plt.ylabel(\"Predicted\")\n",
    "    plt.legend()\n",
    "    plt.suptitle(f\"Drug: {drug_name}\")\n",
    "    results = pd.DataFrame(\n",
    "        [gbt_results, gbt_binary_results, enet_results, lasso_results, svm_results, svc_results]\n",
    "    )\n",
    "    print(results[['model', 'mse', 'r2', 'pearsonr', 'auc']])\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = run_model(\n",
    "    data,\n",
    "    ['wes',],\n",
    "    'Venetoclax',\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = run_model(\n",
    "    data,\n",
    "    ['rna_seq'],\n",
    "    'Venetoclax',\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = run_model(\n",
    "    data,\n",
    "    ['wes', 'phospho', 'proteomics', 'rna_seq'],\n",
    "    'Venetoclax',\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = set(m1['feature_names'].iloc[0])\n",
    "y = set(m1['feature_names'].iloc[2])\n",
    "z = set(m1['feature_names'].iloc[3])\n",
    "hits = x.intersection(y).intersection(z)\n",
    "for i in hits:\n",
    "    print(i.split('-')[0].split('_')[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = run_model(\n",
    "    data,\n",
    "    ['wes', 'phospho', 'proteomics', 'rna_seq'],\n",
    "    'Gilteritinib',\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = run_model(\n",
    "    data,   \n",
    "    ['wes', 'phospho', 'proteomics', 'rna_seq'],\n",
    "    'Quizartinib (AC220)',\n",
    ");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = run_model(\n",
    "    'Elesclomol',\n",
    ");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test bed below\n",
    "Started with xgboost but switched to lightGBM. It seems to be just as accurate but slightly faster. There is a Pytorch model below in the works. Most of below can be ignored for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lightgbm as lgb\n",
    "def run_lgb(sources, drug_name, plot=False):\n",
    "    df_subset = data.get_trainable_data(sources, drug_name)\n",
    "    \n",
    "    cols = list(set(df_subset.columns.values))\n",
    "    cols.remove(drug_name)\n",
    "\n",
    "    features = df_subset[cols].copy()\n",
    "    target = df_subset[drug_name].values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    n_features_before = features.shape[1]\n",
    "\n",
    "    features = features.loc[:, features.mean() > 0]\n",
    "    features = features.loc[:, features.std() > 0]\n",
    "    feature_names = list(set(features.columns.values))\n",
    "    n_features = features.shape[1]\n",
    "    print(f\"Using {n_features} out of {n_features_before}\"\n",
    "          f\" ({n_features_before - n_features} removed)\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features,\n",
    "        target,\n",
    "        test_size=0.2, \n",
    "        shuffle=True, \n",
    "        random_state=101,\n",
    "    )\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    \n",
    "    train_data = lgb.Dataset(X_train, label=y_train, feature_name=feature_names)\n",
    "    validation_data = lgb.Dataset(X_test, label=y_test, feature_name=feature_names, )\n",
    "    \n",
    "#     validation_data = train_data.create_valid('validation.svm')\n",
    "    \n",
    "    param = dict(\n",
    "        device_type='gpu',\n",
    "        boosting='gbdt',\n",
    "        nthread =16,\n",
    "        objective='regression',\n",
    "        metric='rmse',\n",
    "#         lambda_l1=.5,\n",
    "#         lambda_l2=.5,\n",
    "        learning_rate= .01,\n",
    "        tree_learner= 'serial',\n",
    "        max_bin= 128,\n",
    "        num_leaves= 20, \n",
    "        max_depth=6,\n",
    "        feature_fraction= .5,\n",
    "        min_data_in_leaf= 1,\n",
    "        min_gain_to_split=1,\n",
    "        verbose=0\n",
    "        \n",
    "    )\n",
    "\n",
    "    \n",
    "    num_round = 1000\n",
    "    bst = lgb.train(\n",
    "        param, \n",
    "        train_data, \n",
    "        num_round, \n",
    "        valid_sets=validation_data,\n",
    "        callbacks=[lgb.early_stopping(stopping_rounds=100)]\n",
    "    )\n",
    "    bst.save_model('model.txt', num_iteration=bst.best_iteration)\n",
    "#     lgb.plot_importance(bst, figsize =(4, 8))\n",
    "#     plt.show()\n",
    "    \n",
    "    t_preds = bst.predict(X_train, num_iteration=bst.best_iteration)\n",
    "    preds = bst.predict(X_test, num_iteration=bst.best_iteration)\n",
    "    \n",
    "    sns.regplot(y_train, t_preds, label='training')\n",
    "    sns.regplot(y_test, preds, label='prediction')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    error = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"Pearson correlation {pearsonr(y_test, preds)}\")\n",
    "    print(f\"MSE: {error:0.3f} | $R^2$ {r2}\")\n",
    "    return bst\n",
    "drug = 'Venetoclax'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "model = run_lgb('rna_seq', drug)\n",
    "model = run_lgb('phospho', drug)\n",
    "model = run_lgb('proteomics', drug)\n",
    "model = run_lgb(['phospho', 'rna_seq', 'proteomics'], drug)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradient boosted decision trees\n",
    "\n",
    "XGBoost parameters\n",
    "\n",
    "\n",
    "https://xgboost.readthedocs.io/en/latest/parameter.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fit model no training data\n",
    "params = dict(\n",
    "    # general params\n",
    "    nthread=1,\n",
    "    booster='gbtree',\n",
    "    gpu_id=0,\n",
    "    seed=100,\n",
    "    # regularization\n",
    "    reg_alpha=.5, \n",
    "    reg_lambda=5,\n",
    "    num_parallel_tree = 2, \n",
    "#     num_boost_round = 16,\n",
    "    tree_method='gpu_hist',\n",
    "    max_bin=128,\n",
    "    objective='reg:squarederror',\n",
    "    eval_metric='rmse',\n",
    "    learning_rate=0.01,\n",
    "    max_depth=5, \n",
    "    min_child_weight=1,\n",
    "#     gamma=0,\n",
    "    subsample=.5, # use half of data to resample\n",
    "#     colsample_bytree=.8,\n",
    ")\n",
    "def create_importance_model(sources, drug_name, plot=False):\n",
    "    df_subset = data.get_trainable_data(sources, drug_name)\n",
    "    \n",
    "    cols = list(set(df_subset.columns.values))\n",
    "    cols.remove(drug_name)\n",
    "\n",
    "    features = df_subset[cols].copy()\n",
    "    target = df_subset[drug_name].values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    n_features_before = features.shape[1]\n",
    "\n",
    "    features = features.loc[:, features.mean() > 0]\n",
    "    features = features.loc[:, features.std() > 0]\n",
    "    feature_names = list(set(features.columns.values))\n",
    "    n_features = features.shape[1]\n",
    "    print(f\"Using {n_features} out of {n_features_before}\"\n",
    "          f\" ({n_features_before - n_features} removed)\")\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        features,\n",
    "        target,\n",
    "        test_size=0.2, \n",
    "        shuffle=True, \n",
    "        random_state=101,\n",
    "    )\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    \n",
    "    # organize data into xgb data matrix\n",
    "    train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "    test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "    # add gene names as feature labels\n",
    "    train.feature_names = feature_names\n",
    "    test.feature_names = feature_names\n",
    "    \n",
    "    num_round = 10000\n",
    "    results = dict()\n",
    "    model = xgb.train(\n",
    "        params, train, num_round,\n",
    "        verbose_eval=100,\n",
    "        early_stopping_rounds=100,\n",
    "        evals=[(train, 'train'), (test, 'valid')],\n",
    "        evals_result=results,\n",
    "        \n",
    "    )  \n",
    "    \n",
    "    feature_scores = model.get_fscore()\n",
    "    s = pd.Series(list(feature_scores.values()), index=feature_scores)\n",
    "    print(s.sort_values(ascending=False).head(5))\n",
    "    \n",
    "    #trained \n",
    "    t_preds = model.predict(train, iteration_range=(0, model.best_iteration + 1))\n",
    "\n",
    "    #predictions\n",
    "    preds = model.predict(test)\n",
    "    error = np.sqrt(mean_squared_error(y_test, preds))\n",
    "    r2 = r2_score(y_test, preds)\n",
    "    print(f\"MSE: {error:0.3f}\")\n",
    "    print(f\"$R^2$ {r2}\\n\")\n",
    "    if plot:\n",
    "        # create plot of training and performance\n",
    "        # plot training over time\n",
    "        x_axis = range(0, len(results['train']['rmse']))\n",
    "        plt.figure(figsize=(12, 6))\n",
    "        plt.subplot(121)\n",
    "        plt.plot(x_axis, results['train']['rmse'], label='Train')\n",
    "        plt.plot(x_axis, results['valid']['rmse'], label='Test')\n",
    "        plt.legend()\n",
    "\n",
    "        # plot projections vs actual\n",
    "        plt.subplot(122)\n",
    "        sns.regplot(y_train, t_preds, label='training')\n",
    "        sns.regplot(y_test, preds, label='prediction')\n",
    "        plt.legend()\n",
    "        plt.suptitle(f\"{sources} {drug_name} : RMSE = {error}, $r^2$= {r2}\")\n",
    "    if not isinstance(sources, str):\n",
    "        out_name = '_'.join(sorted(sources))\n",
    "    else:\n",
    "        out_name = sources\n",
    "    return {\n",
    "        'model': model,\n",
    "        'data_sets': out_name,\n",
    "        'drug_name': drug_name,\n",
    "        'mse': error,\n",
    "        'r2': r2\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = 'Venetoclax'\n",
    "model = create_importance_model('phospho', drug, True)['model']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = model.get_score(importance_type='total_gain')\n",
    "feature_scores = pd.Series(list(feature_scores.values()), index=feature_scores)\n",
    "feature_scores.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "drug = 'Venetoclax'\n",
    "model = create_importance_model('rna_seq', drug, True)['model']\n",
    "feature_scores = model.get_score(importance_type='total_gain')\n",
    "feature_scores = pd.Series(list(feature_scores.values()), index=feature_scores)\n",
    "feature_scores.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scores = model.get_score(importance_type='gain')\n",
    "feature_scores = pd.Series(list(feature_scores.values()), index=feature_scores)\n",
    "feature_scores.sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "drug = 'Quizartinib (AC220)'\n",
    "create_importance_model('phospho', drug, True)\n",
    "create_importance_model('proteomics', drug, True)\n",
    "create_importance_model('rna_seq', drug, True)\n",
    "create_importance_model(['phospho', 'rna_seq', 'proteomics'], drug, True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d_name = 'Venetoclax'\n",
    "def run_input_sets(drug_name):\n",
    "    output = []\n",
    "    input_sets = [\n",
    "        'rna_seq', 'proteomics', 'phospho',\n",
    "#         ['rna_seq', 'proteomics'],\n",
    "#         ['rna_seq', 'phospho'],\n",
    "#         ['proteomics', 'phospho'],\n",
    "#         ['rna_seq', 'proteomics', 'phospho'],\n",
    "    ]\n",
    "    for i in input_sets:\n",
    "        result = create_importance_model(i, drug_name, True)\n",
    "        del result['model']\n",
    "        output.append(result)\n",
    "    return output\n",
    "def run_for_all():\n",
    "    all_output= []\n",
    "    for i in data.drug_names[:2]:\n",
    "        x = run_input_sets(i)\n",
    "        all_output.append(x)\n",
    "    all_output = list(itertools.chain.from_iterable(all_output))\n",
    "    return all_output\n",
    "df = run_for_all()\n",
    "df.to_csv('gbt_all_drugs_all_dtypes.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "all_output = pd.DataFrame(all_output)\n",
    "# del all_output['model']\n",
    "all_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold \n",
    "K-fold implementation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_kfold(sources, drug_name):\n",
    "    df_subset = data.get_trainable_data(sources, drug_name)\n",
    "    \n",
    "    cols = list(set(df_subset.columns.values))\n",
    "    cols.remove(drug_name)\n",
    "\n",
    "    features = df_subset[cols].copy()\n",
    "    target = df_subset[drug_name].values.reshape(-1, 1).ravel()\n",
    "    \n",
    "    n_features_before = features.shape[1]\n",
    "    features = features.loc[:, features.mean() > 0]\n",
    "    features = features.loc[:, features.std() > 0]\n",
    "    feature_names = list(set(features.columns.values))\n",
    "    n_features = features.shape[1]\n",
    "    print(f\"Using {n_features} out of {n_features_before}\"\n",
    "          f\" ({n_features_before - n_features} removed)\")\n",
    "    \n",
    "    # define data_dmatrix\n",
    "    data_dmatrix = xgb.DMatrix(data=features, label=target)\n",
    "    \n",
    "    features = features.values\n",
    "    \n",
    "    # way 1\n",
    "    features_output = []\n",
    "    output_tracker = []\n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "    for n, (train_index, test_index) in enumerate(kf.split(features)):\n",
    "        X_train, X_test = features[train_index], features[test_index]\n",
    "        y_train, y_test = target[train_index], target[test_index]\n",
    "\n",
    "        X_train = scaler.fit_transform(X_train)\n",
    "        X_test = scaler.fit_transform(X_test)\n",
    "\n",
    "        # organize data into xgb data matrix\n",
    "        train = xgb.DMatrix(data=X_train, label=y_train)\n",
    "        test = xgb.DMatrix(data=X_test, label=y_test)\n",
    "\n",
    "        # add gene names as feature labels\n",
    "        train.feature_names = feature_names\n",
    "        test.feature_names = feature_names\n",
    "        \n",
    "        num_round = 10000\n",
    "        results = dict()\n",
    "        model = xgb.train(\n",
    "            params, train, num_round,\n",
    "            verbose_eval=100,\n",
    "            early_stopping_rounds=100,\n",
    "            evals=[(train, 'train'), (test, 'valid')],\n",
    "            evals_result=results,\n",
    "\n",
    "        )  \n",
    "\n",
    "        feature_scores = model.get_fscore()\n",
    "        s = pd.Series(list(feature_scores.values()), index=feature_scores)\n",
    "       \n",
    "        s = s.to_frame().reset_index()\n",
    "        s.rename(columns= {0: 'count'}, inplace=True)\n",
    "        s.index.name = 'feature'\n",
    "        s['nfold'] = n\n",
    "        features_output.append(s)\n",
    "\n",
    "        #trained \n",
    "        t_preds = model.predict(train, iteration_range=(0, model.best_iteration + 1))\n",
    "        #predictions\n",
    "        preds = model.predict(test)\n",
    "        \n",
    "        \n",
    "        error = np.sqrt(mean_squared_error(y_test, preds))\n",
    "        r2 = r2_score(y_test, preds)\n",
    "        print(f\"MSE: {error:0.3}\")\n",
    "        print(f\"$R^2$ {r2}\")\n",
    "        output_tracker.append({'nfold':n, 'r2': r2, 'error':error})\n",
    "    return pd.DataFrame(output_tracker), pd.concat(features_output, ignore_index=True)\n",
    "\n",
    "#      way 2\n",
    "#     kf = KFold(n_splits=5, shuffle=True, random_state=101)\n",
    "#     for train_index, test_index in kf.split(features):\n",
    "#         X_train, X_test = features[train_index], features[test_index]\n",
    "#         y_train, y_test = target[train_index], target[test_index]\n",
    "        \n",
    "#         xgb_model = xgb.XGBRegressor(**params).fit(\n",
    "#             X_train, y_train,\n",
    "#             early_stopping_rounds=100,\n",
    "#             eval_set=[(X_test, y_test)],\n",
    "#             verbose=100,\n",
    "            \n",
    "#         )\n",
    "        \n",
    "#         print(xgb_model.feature_importances_)\n",
    "#         predictions = xgb_model.predict(X_test)\n",
    "#         print(mean_squared_error(y_test, predictions))\n",
    "#      way 3\n",
    "#     xgb_cv = cv(\n",
    "#         dtrain=data_dmatrix,\n",
    "#         params=params, \n",
    "#         nfold=5,\n",
    "#         num_boost_round=50, \n",
    "#         early_stopping_rounds=10, \n",
    "#         metrics=[\"rmse\"], \n",
    "#         as_pandas=True,\n",
    "#         seed=123\n",
    "#     )\n",
    "#     return xgb_cv\n",
    "    \n",
    "    \n",
    "stats, feat_out = run_kfold(['phospho', 'rna_seq', 'proteomics'], 'Venetoclax')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "total_selects = feat_out.groupby('index').sum()['count']\n",
    "filtered = total_selects.sort_values(ascending=False).head(10)\n",
    "sorted_index = list(filtered.index)\n",
    "subset = feat_out.loc[feat_out['index'].isin(sorted_index)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.barplot(x='count', y='index', data=subset, orient='h', order=sorted_index);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "g = sns.catplot(x=\"count\", y=\"index\",\n",
    "                hue=\"nfold\", \n",
    "                data=subset, kind=\"bar\", order=sorted_index,\n",
    "                height=8, aspect=1);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "models = []\n",
    "for i in ['Venetoclax', 'Gilteritinib', 'Quizartinib (AC220)',\n",
    "              'Trametinib (GSK1120212)', 'Sorafenib']:\n",
    "    models.append(create_importance_model('rna_seq', i, False))\n",
    "    models.append(create_importance_model('proteomics', i, False))\n",
    "    models.append(create_importance_model('phospho', i, False))\n",
    "    models.append(\n",
    "        create_importance_model(\n",
    "            ['phospho', 'rna_seq', 'proteomics'], i, False\n",
    "        )\n",
    "    )\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(models, columns=['rmse', 'r2', 'data_type', 'drug'])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = sns.swarmplot(data=df, x=\"r2\", y=\"drug\", hue=\"data_type\")\n",
    "ax.set(ylabel=\"\");\n",
    "ax.set(xlabel=\"$R^2$\");"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_plot_data = data.get_trainable_data(['rna_seq'], 'Sorafenib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ex_plot_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.kdeplot(ex_plot_data['GMPPB_prot'], ex_plot_data['Sorafenib'])\n",
    "sns.kdeplot(ex_plot_data['BCL9L_prot'], ex_plot_data['Sorafenib'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NN\n",
    "Keeping for the moment..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from tqdm.notebook import tqdm\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, D_in, H1, D_out):\n",
    "        super(Net, self).__init__()\n",
    "        self.linear_stack = nn.Sequential(\n",
    "            nn.Linear(D_in, H1,),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(H1, H1),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(H1, D_out),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        y_pred = self.linear_stack(x)\n",
    "        return y_pred\n",
    "    \n",
    "\n",
    "import time\n",
    "from copy import deepcopy\n",
    "def run_nn(save_name, drug_name, verbose=False):\n",
    "    torch.cuda.empty_cache()\n",
    "    dev = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    dtype = torch.float32\n",
    "    \n",
    "    df_subset = data.get_trainable_data(['phospho', 'proteomics', 'rna_seq'], drug_name)\n",
    "    \n",
    "    cols = list(set(df_subset.columns.values))\n",
    "    cols.remove(drug_name)\n",
    "\n",
    "    features = df_subset[cols].copy()\n",
    "    target = df_subset[drug_name].values.reshape(-1, 1)\n",
    "    n_features_before = features.shape[1]\n",
    "\n",
    "    features = features.loc[:, features.mean() > 0]\n",
    "    features = features.loc[:, features.std() > 0]\n",
    "    n_features = features.shape[1]\n",
    "    print(f\"Using {n_features} out of {n_features_before}\"\n",
    "          f\" ({n_features_before - n_features} removed)\")\n",
    "    \n",
    "    try:\n",
    "        X_train, X_test, y_train, y_test = train_test_split(\n",
    "            features,\n",
    "            target,\n",
    "            test_size=0.2, \n",
    "            shuffle=True, \n",
    "            random_state=69,\n",
    "        )\n",
    "    except:\n",
    "        print(\"error\")\n",
    "        return\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler()\n",
    "    X_train = scaler.fit_transform(X_train)\n",
    "    X_test = scaler.fit_transform(X_test)\n",
    "    \n",
    "    \n",
    "    X_train = torch.tensor(X_train, dtype=dtype,  device=dev)\n",
    "    X_test = torch.tensor(X_test, dtype=dtype,  device=dev)\n",
    "\n",
    "    y_train = torch.tensor(y_train, dtype=dtype,  device=dev)\n",
    "    y_test = torch.tensor(y_test, dtype=dtype,  device=dev)\n",
    "\n",
    "    D_in, D_out = X_train.shape[1], y_train.shape[1]\n",
    "    H1 = int((D_in+D_out)/2)\n",
    "\n",
    "    model = Net(D_in, H1, D_out)\n",
    "    model.to(dev)\n",
    "    model.to(dtype)\n",
    "    \n",
    "    criterion = nn.MSELoss(reduction='mean')\n",
    "    \n",
    "#     optimizer = torch.optim.SGD(model.parameters(), lr=1e-4)\n",
    "    \n",
    "    optimizer = torch.optim.Adam(\n",
    "        model.parameters(),\n",
    "        lr=1e-5,\n",
    "        betas=(0.9, 0.999), \n",
    "        eps=1e-08, \n",
    "        weight_decay=0,\n",
    "        amsgrad=False\n",
    "    )\n",
    "#     optimizer = torch.optim.SGD(\n",
    "#         model.parameters(),\n",
    "#         lr=0.001,\n",
    "#         momentum=0.5\n",
    "#     )\n",
    "#     optimizer = torch.optim.ASGD(\n",
    "#         model.parameters(), lr=1e-5, lambd=0.001, \n",
    "#         alpha=0.0, t0=1000.0, weight_decay=0.\n",
    "#     )\n",
    "\n",
    "    n_epoch = 10000\n",
    "    eval_per_epoch = n_epoch//100\n",
    "    prev_loss = np.inf\n",
    "    steps_wo_impro = 0\n",
    "    best_state = deepcopy(model.state_dict())\n",
    "    best_epoch = 0\n",
    "    st = time.time()\n",
    "    train, valid = [], []\n",
    "    p_bar = tqdm(range(n_epoch))\n",
    "    for epoch in range(n_epoch):\n",
    "        if steps_wo_impro > eval_per_epoch*10:\n",
    "#             print(f\"Early stopping at {epoch}\")\n",
    "            break\n",
    "        model.train()\n",
    "        # use original method\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(X_train)\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        if torch.isnan(loss):\n",
    "            break        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        with torch.no_grad():\n",
    "        \n",
    "            test_loss = criterion(model(X_test), y_test).item()\n",
    "\n",
    "            train.append(loss.item())\n",
    "            valid.append(test_loss)\n",
    "\n",
    "            if test_loss < prev_loss:\n",
    "                prev_loss = deepcopy(test_loss)\n",
    "                steps_wo_impro = 0\n",
    "                best_state = deepcopy(model.state_dict())\n",
    "                best_epoch = epoch\n",
    "            else:\n",
    "                steps_wo_impro += 1\n",
    "    #         if epoch < 1000:\n",
    "    #             steps_wo_impro = 0\n",
    "    #             prev_loss = np.inf\n",
    "            if verbose:\n",
    "                if (epoch+1) % eval_per_epoch == 0:\n",
    "                    p_bar.set_description(\n",
    "                        'Epoch [{}/{}], Loss train: {:2.4e}, test {:2.4e}'.format(\n",
    "                            epoch+1, n_epoch, loss.item(), test_loss)\n",
    "                    )\n",
    "#     print(f'time : {time.time()-st}')\n",
    "    del model\n",
    "    \n",
    "    test_model = Net(D_in, H1, D_out)\n",
    "    test_model.load_state_dict(best_state)\n",
    "    test_model.to(dtype)\n",
    "    test_model.to(dev)\n",
    "    test_model.eval()\n",
    "    \n",
    "    train_pred = test_model(X_train).cpu().detach().numpy()\n",
    "    train_actual = y_train.cpu().detach().numpy()\n",
    "    \n",
    "    test_pred = test_model(X_test).cpu().detach().numpy()\n",
    "    test_actual =  y_test.cpu().numpy()\n",
    "    error = mean_squared_error(test_pred, test_actual)\n",
    "\n",
    "    error = \"{0:.3f}\".format(np.sqrt(error))\n",
    "    r2 = r2_score(np.array(test_actual).flatten(), \n",
    "                  np.array(test_pred).flatten())\n",
    "    print(f\"\\t {error}\")  \n",
    "    print(f\"r2 = {r2}\")  \n",
    "    \n",
    "    # simple plot\n",
    "    plt.figure(figsize=(12, 6))\n",
    "    plt.subplot(121)\n",
    "    x_axis = range(0, len(train))\n",
    "    plt.plot(best_epoch, np.sqrt(prev_loss), marker='^')\n",
    "    plt.plot(x_axis, np.sqrt(train), label='Train')\n",
    "    plt.plot(x_axis, np.sqrt(valid), label='Test')\n",
    "    plt.legend()\n",
    "\n",
    "    plt.subplot(122)\n",
    "    sns.regplot(train_pred[:,0], train_actual[:,0], label='Training')\n",
    "    sns.regplot(test_pred[:,0], test_actual[:,0], label='Prediction')\n",
    "    plt.legend()\n",
    "    plt.suptitle(f\"{drug_name} {save_name} : RMSE = {error} r^2 {r2:.2f}\")\n",
    "#     plt.close()\n",
    "    return np.sqrt(mean_squared_error(test_pred, test_actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "run_nn('Venetoclax', 'Venetoclax', True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
